# -*- coding: utf-8 -*-
"""Maternal_Health_Risk_Analysis.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Uy8rMRKbXuDcmVvZd6jGVIf3sEmem1M4
"""

!pip install gradio --quiet

# Import libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve
from sklearn.ensemble import RandomForestClassifier
import joblib

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import gradio as gr

from google.colab import files
uploaded = files.upload()

df = pd.read_csv("Maternal Health Risk Data Set (1).csv")
df.head()

# exploring the dataset
df.shape

# Checking the column names
df.columns

# Checking the missing values
df.isnull().sum()

# Checking the class balance
df["RiskLevel"].value_counts()

# Data cleaning
df = df.dropna()

# Check if any missing values remain
df.isnull().sum()

df.shape

# Separate features and targets
X = df.drop("RiskLevel", axis=1)
y = df["RiskLevel"]

X.head()

# Verify Target vector
y.head()

print("Feature shape:", X.shape)
print("Target shape:", y.shape)

# Identify the features and target
X = df.drop("RiskLevel", axis=1)
y = df["RiskLevel"]

# Train test split
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(
    X, y,
    test_size=0.2,
    stratify=y,
    random_state=42
)

# Training the Random forest without scaling
from sklearn.ensemble import RandomForestClassifier

rf_model = RandomForestClassifier(
    n_estimators=100,
    random_state=42
)

rf_model.fit(X_train, y_train)

# Predictions
y_pred = rf_model.predict(X_test)
y_prob = rf_model.predict_proba(X_test)

# Eavaluate the performance
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

print("Accuracy:", accuracy_score(y_test, y_pred))
print("Precision:", precision_score(y_test, y_pred, average='weighted'))
print("Recall:", recall_score(y_test, y_pred, average='weighted'))
print("F1-score:", f1_score(y_test, y_pred, average='weighted'))

# ROC (AUC curve)
from sklearn.preprocessing import label_binarize
from sklearn.metrics import roc_curve, auc
import matplotlib.pyplot as plt

classes = rf_model.classes_
y_test_bin = label_binarize(y_test, classes=classes)

for i in range(len(classes)):
    fpr, tpr, _ = roc_curve(y_test_bin[:, i], y_prob[:, i])
    roc_auc = auc(fpr, tpr)
    plt.plot(fpr, tpr, label=f"{classes[i]} (AUC = {roc_auc:.2f})")

plt.plot([0,1], [0,1], linestyle='--')
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("ROC Curve - Random Forest")
plt.legend()
plt.show()

# Feature importance
import pandas as pd

feature_importances = pd.Series(
    rf_model.feature_importances_,
    index=X.columns
).sort_values(ascending=False)

print(feature_importances)

feature_importances.plot(kind='bar')
plt.title("Feature Importance")
plt.show()

# Save the model
import joblib
joblib.dump(rf_model, "risk_model.joblib")

# Part 3 - Deep neural network
# Scaling the data
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()

X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Build neural network
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers

num_classes = len(y.unique())

model = keras.Sequential([
    layers.Dense(64, activation='relu', input_shape=(X_train_scaled.shape[1],)),
    layers.Dense(32, activation='relu'),
    layers.Dense(num_classes, activation='softmax')
])

model.compile(
    optimizer='adam',
    loss='sparse_categorical_crossentropy',
    metrics=['accuracy']
)

# Checking the data types
print(X_train_scaled.dtype)
print(y_train.dtype)

X = df.drop("RiskLevel", axis=1)   # change column name if different
y = df["RiskLevel"]

from sklearn.preprocessing import LabelEncoder

le = LabelEncoder()
y_encoded = le.fit_transform(y)

from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(
    X_scaled,
    y_encoded,
    test_size=0.2,
    random_state=42
)

history = model.fit(
    X_train,
    y_train,
    epochs=50,
    batch_size=16,
    validation_split=0.2
)

test_loss, test_accuracy = model.evaluate(X_test, y_test)

print("Test Loss:", test_loss)
print("Test Accuracy:", test_accuracy)

test_loss, test_accuracy = model.evaluate(X_test, y_test)

print("Test Loss:", test_loss)
print("Test Accuracy:", test_accuracy)

# for 3 class classification we use softmax
layers.Dense(3, activation='softmax')

model.compile(
    optimizer='adam',
    loss='sparse_categorical_crossentropy',
    metrics=['accuracy']
)

# Making predictions

y_pred_probs = model.predict(X_test)
y_pred = np.argmax(y_pred_probs, axis=1)

# Getting metrics
from sklearn.metrics import classification_report

print(classification_report(y_test, y_pred))

# Save it
model.save("risk_model_dnn.keras")

# The Dense Neural Network was trained for 50 epochs using Adam optimizer.
# The model achieved 66% accuracy on the test dataset.
# Class 0 was predicted most accurately with high recall (0.91).
# The model struggled to correctly identify Class 2, with lower recall (0.38).
# Overall weighted F1-score is 0.64, indicating moderate performance.
# The trained model was saved as risk_model_dnn.keras.

#  Part 4 - Creating Gradio app

import gradio as gr
import joblib
import numpy as np

# Load trained Random Forest model
model = joblib.load("risk_model.joblib")

# Prediction function
def predict_risk(age, systolic_bp, diastolic_bp, blood_sugar, body_temp, heart_rate):

    # Create input array (must match training feature order)
    input_data = np.array([[age, systolic_bp, diastolic_bp, blood_sugar, body_temp, heart_rate]])

    prediction = model.predict(input_data)[0]

    # Convert numeric label back to risk category
    if prediction == 0:
        return "Low Risk"
    elif prediction == 1:
        return "Mid Risk"
    else:
        return "High Risk"

# Build Gradio interface
interface = gr.Interface(
    fn=predict_risk,
    inputs=[
        gr.Number(label="Age"),
        gr.Number(label="Systolic Blood Pressure"),
        gr.Number(label="Diastolic Blood Pressure"),
        gr.Number(label="Blood Sugar"),
        gr.Number(label="Body Temperature"),
        gr.Number(label="Heart Rate")
    ],
    outputs=gr.Textbox(label="Predicted Maternal Risk Level"),
    title="AI-Based Maternal Risk Scoring System",
    description="Enter patient details to predict maternal risk level."
)

if __name__ == "__main__":
    interface.launch()